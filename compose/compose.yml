name: llm-stack

services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    platform: linux/arm64
    container_name: litellm
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      TELEMETRY: "false"
    entrypoint: "litellm"
    command: ["--port", "4000", "--config", "/app/config.yaml"]
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
    restart: unless-stopped
    networks:
      - internal
    expose:
      - "4000"

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    platform: linux/arm64
    container_name: openwebui
    environment:
      OPENAI_API_BASE_URL: "http://litellm:4000"
      OPENAI_API_KEY: "anything"
    volumes:
      - openwebui-data:/app/backend/data
    depends_on:
      - litellm
    restart: unless-stopped
    networks:
      - internal
    expose:
      - "8080"

  caddy:
    image: caddy:2-alpine
    platform: linux/arm64
    container_name: caddy
    environment:
      CADDY_DOMAIN: ${CADDY_DOMAIN}
      ACME_EMAIL: ${ACME_EMAIL}
    volumes:
      - ../caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data
      - caddy-config:/config
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - openwebui
    restart: unless-stopped
    networks:
      - internal

networks:
  internal:
    driver: bridge

volumes:
  openwebui-data:
  caddy-data:
  caddy-config: